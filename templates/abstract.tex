% $Log: abstract.tex,v $
% Revision 1.1  93/05/14  14:56:25  starflt
% Initial revision
% 
% Revision 1.1  90/05/04  10:41:01  lwvanels
% Initial revision
% 
%
%% The text of your abstract and nothing else (other than comments) goes here.
%% It will be single-spaced and the rest of the text that is supposed to go on
%% the abstract page will be generated by the abstractpage environment.  This
%% file should be \input (not \include 'd) from cover.tex.
If we are to take a computational approach to understand the human natural language faculty, we must begin by choosing computational models that, at least in spirit, align with the properties and constraints of human Knowledge of Language (KoL).  Ideally, this would provide the same competence as a native speaker.  This thesis takes three critical steps toward demonstrating that at least one necessary step in this direction is to choose computational models that are fundamentally different from Artificial Neural Networks (NNs).  

First and most importantly, this thesis sets out a particular null hypothesis ($\mathrm{H}_0$) to be used as common ground for all computational KoL research: at the time of writing there exists no language model or nonhuman computational device that exhibits human comparable KoL.  This thesis grounds the significance of this null hypothesis, and why this must be the working hypothesis against which all claims are made, in the conventional definition of a Language Model as a probabilistic generative device for approximating the input-output behavior of human language performance.  More specifically, this thesis shows that NNs, despite their status as Universal Function Approximators, still fail to approximate an otherwise deterministic system such as Natural Language due to Language's property of Digital Infinity.

After examining this fundamental limitation of NNs, this thesis will assume that Natural Language may be empirically approximated by a probabilistic system, in order to demonstrate how that assumption is not supported by state-of-the-art NN systems.  In particular, this thesis probes the following claim ($\mathrm{H}_\mathrm{A}$): Bidirectional Encoder Representations from Transformers (henceforth referred to as BERT) (Devlin et al. 2018) possess syntactic knowledge comparable to that of humans.\footnote{For a recent review of the knowledge of language that has been attributed to BERT, see \textit{A Primer in BERTology: What We Know About How BERT Works}, (Rogers et al. 2020)} It follows from this that BERT's Masked Language Modeling (MLM) predictions, pretrained on 3300M tokens of continuous text, can successfully track human acceptability judgements, in which the grammaticality of each sentence is presumed to play a central, causal role in that sentence's acceptability judgement.  In order to isolate grammaticality further as the cause of a sentence's acceptability, we use two sets of sentences and human judgement data collected by Sprouse & Almeida, 2012, and Sprouse et al. 2013, that are representative of the field of generative syntax (Adger's \textit{Core Syntax}, 2003), have shaped current theories in generative syntax, and have been shown to be statistically powerful and reliably replicable.  We find that BERT's MLM predictions only have a weak positive correlation with human acceptability judgements (0.30 and 0.32 on two datasets).

Next, we conduct a similar case study with BERT, this time fine-tuning it using the Corpus of Linguistic Acceptability (CoLA, Warstadt et al. 2018).  Fine-tuning using CoLA enables BERT to use the representations it learned during pre-training by adding a linear network output layer to classify sequences of words as acceptable (1) or unacceptable (0), in this way yielding improved acceptability judgements.  We then again compare these results to human judgement data, resulting in improved positive correlations (0.57 and 0.70 on two datasets).  However, we also uncover irregularities in the behavior of BERT's judgements, such as sensitivity to the order in which the data is presented to fine-tune BERT.  The fact that BERT may classify sentences as acceptable or not according to the order in which the fine-tuning data is presented is striking evidence that BERT's acceptability judgements do not track native speakers' acceptability judgements.  Native speakers of the same language are exposed to vastly different linguistic environments during their language acquisition stages, yet human acceptability judgements of fixed sentences follow consistent trends from speaker to speaker, whereas BERT's predictions follow no consistent nor predictable pattern, even though the linguistic data used to pre-train and fine-tune it remains exactly the same.  This is therefore also evidence in favor of $\mathrm{H}_0$: BERT does not yet possess human comparable KoL, and is instead relying on nonlinguistic information to make its predictions.

Finally, this thesis demonstrates via a final case study why the claim ($\mathrm{H}_\mathrm{A}$) has not yet been established in spite of multiple studies showing the reverse, that BERT does has knowledge of different properties of syntax, such as Subject-Verb Agreement.  To do this, we \textit{prime} BERT on 1 out of 5 syntactic phenomena from the Lasnik & Uriagereka 2003 textbook \textit{A Course in Minimalist Syntax} and show performance improvements of over 10\% classification accuracy as a result of only 20 new training sentences.  The experimental setup reveals learning dynamics that diverge from that of humans, whose acceptability judgements do not swing drastically even when primed toward a particular syntactic structure or phenomenon.  The peculiarities of the results provide further evidence in favor of the null hypothesis.

In short, this thesis underscores the importance of $\mathrm{H}_0$ and its role as the starting assumption of any and all computational linguistics experiments with Language Models.  By having $\mathrm{H}_0$ as a common starting assumption across experimenters, and by using the well attested and validated human judgement data presented in this thesis as the gold standard by which all Language Models should be measured, we will better understand the limitations and necessary compromises in performance and accuracy that need to be made in order to use NNs to model Natural Language.  Most importantly, this thesis asserts the need to turn to a different class of computational models, if one wants to build models with human-like KoL, or gain insight into KoL.
