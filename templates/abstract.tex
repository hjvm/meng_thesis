% $Log: abstract.tex,v $
% Revision 1.1  93/05/14  14:56:25  starflt
% Initial revision
% 
% Revision 1.1  90/05/04  10:41:01  lwvanels
% Initial revision
% 
%
%% The text of your abstract and nothing else (other than comments) goes here.
%% It will be single-spaced and the rest of the text that is supposed to go on
%% the abstract page will be generated by the abstractpage environment.  This
%% file should be \input (not \include 'd) from cover.tex.
If we are to take a computational approach to understand the human natural language faculty, we must begin by choosing computational models that, at least in spirit, observe the properties and constraints that give rise to the observed use of Natural Language.  A good model of Knowledge of Language (KoL) exhibits the same competence as a native speaker, and this thesis document takes three critical steps toward demonstrating the need for a choice of computational models that is fundamentally different from Artificial Neural Networks (NNs).  

First and foremost, this thesis sets out a particular null hypothesis ($\mathrm{H}_0$): at the time of writing there exists no language model or nonhuman computational device that exhibits human comparable KoL.  This thesis takes further steps in grounding the significance of this null hypothesis, and why it must be disproved, by examining the conventional definition of a Language Model as a probabilistic device for approximating the input-output behavior of human language performance.  More specifically, it shows that NNs, despite their status as Universal Function Approximators, still fail to approximate an otherwise deterministic system such as Natural Language due to Language's property of Digital Infinity.

After examining this fundamental limitation of NNs, this thesis will assume that Natural Language may be empirically approximated by a probabilistic system, in order to demonstrate how that assumption is not supported by the state-of-the-art.  In particular, this thesis probes the following claim ($\mathrm{H}_\mathrm{A}$): Bidirectional Encoder Representations from Transformers (henceforth referred to as BERT) (Devlin et al. 2018) possess syntactic knowledge comparable to that of humans.\footnote{For a recent review of the knowledge of language that has been attributed to BERT, see \textit{A Primer in BERTology: What We Know About How BERT Works}, (Rogers et al. 2020)} It follows that BERT's Masked Language Modeling (MLM) predictions, pretrained on 3300M tokens of continuous text, can successfully track human acceptability judgements, in which the grammaticality of each sentence is presumed to play a central, causal role in its acceptability judgement.  In order to isolate grammaticality further as the cause of a sentence's acceptability (or lack thereof), here we use two sets of sentences and human judgement data collected by Sprouse & Almeida, 2012, and Sprouse et al. 2013, that are representative of the field of generative syntax (Adger's \textit{Core Syntax}, 2003), have shaped current theories in generative syntax, and have been shown to be statistically powerful and reliably replicable experiments.  We find that BERT's MLM predictions only have a weak positive correlation with human acceptability judgements (0.30 and 0.32 on two datasets).

Next, we conduct a similar case study with BERT, this time fine-tuning it using the Corpus of Linguistic Acceptability (CoLA, Warstadt et al. 2018).  Fine-tuning using CoLA will enable BERT to use the representations it learned during pretraining by training a linear output layer to classify sequences of words as acceptable (1) or unacceptable (0), thus yielding improved acceptability judgements.  We then again compare the resulting acceptability judgements to human judgement data, along with further irregularities in the behavior of BERT's judgements, such as unstable acceptability judgements sensitive to the order in which the data used to fine-tune BERT was presented, in spite of the improved positive correlation (0.57 and 0.70 on two datasets).  The fact that BERT may classify sentences as acceptable or not according to the order in which the fine-tuning data was presented is strong evidence that BERT's acceptability judgements are not tracking native speakers' acceptability judgements.  This is therefore also evidence in favor of $\mathrm{H}_0$: BERT does not yet possess human comparable KoL.

Lastly, this thesis demonstrates via a final case study why the claim ($\mathrm{H}_\mathrm{A}$) has not yet been proved in spite of multiple studies showing the reverse, that BERT does has knowledge of different properties of syntax, such as Subject-Verb Agreement.  To do this, we \textit{prime} BERT on 1 out of 5 syntactic phenomena from the Lasnik & Uriagereka 2003 textbook \textit{A Course in Minimalist Syntax} and show performance improvements of over 10\% classification accuracy as a result of only 20 new training sentences.  The experimental setup reveals learning dynamics that diverge from that of humans, whose acceptability judgements do not swing drastically even when primed toward a particular syntactic structure or phenomenon.  The peculiarities of the results provide further evidence in favor of the null hypothesis.

This thesis underscores the importance of $\mathrm{H}_0$ and its role as the starting assumption of any and all computational linguistics experiments with Language Models.  By having $\mathrm{H}_0$ as a common starting assumption across experimenters, and by using the well attested and validated human judgement data presented in this thesis as the gold standard by which all Language Models should be measured, we will better understand the limitations and necessary compromises in performance and accuracy that need to be made in order to use NNs to model Natural Language.  Most importantly, this thesis asserts the need to turn to a different class of computational models outright, if one wants to build models with human-like KoL, or even glean useful insights into Natural Language, that is.
