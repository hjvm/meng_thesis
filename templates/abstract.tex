% $Log: abstract.tex,v $
% Revision 1.1  93/05/14  14:56:25  starflt
% Initial revision
% 
% Revision 1.1  90/05/04  10:41:01  lwvanels
% Initial revision
% 
%
%% The text of your abstract and nothing else (other than comments) goes here.
%% It will be single-spaced and the rest of the text that is supposed to go on
%% the abstract page will be generated by the abstractpage environment.  This
%% file should be \input (not \include 'd) from cover.tex.
If we are to take a computational approach to understand the human natural language faculty, we must first start by choosing computational models that, at least in spirit, observe the constraints and optimizations that led to the birth of human natural language.  In this thesis document, I take three critical steps toward demonstrating the need for a choice of computational models fundamentally different from Artificial Neural Networks (NNs).  

First and foremost, I define the null hypothesis ($\mathrm{H}_0$): at the time of writing the exists no language model nor computational device that exhibits Knowledge of Syntax comparable to humans.  In other words, we humans are the only entity known to us that possess Knowledge of Syntax.  I take further steps in grounding the significance of the null hypothesis, and why it must be disproved, by examining the definition of a Language Model as a probabilistic device.  More specifically, how Neural Language Models, in spite of the Universal Function Approximation assumption, will fail to approximate an otherwise deterministic system such as Natural Language due to its property of Digital Infinity.  I address this argument almost in its entirety in Chapter 1.  

Secondly, I assume that Natural Language may indeed be approximated by a probabilistic system.  In particular, I assert the following claim ($\mathrm{H}_{A}$): Bidirectional Encoder Representations from Transformers (henceforth referred to as BERT, Devlin et al. 2018) possess syntactic knowledge comparable to that of humans.\footnote{For a recent review of the knowledge of language that has been attributed to BERT, see \textit{A Primer in BERTology: What We Know About How BERT Works}, (Rogers et al. 2020)} It follows that BERT's Masked Language Modeling (MLM) predictions, after having been pretrained on 3300M tokens of continuous text, will successfully track human acceptability judgements, in which the grammaticality of each sentence plays a vital role in the final judgement.  In order to isolate grammaticality further as the cause of a sentence's acceptability (or lack thereof), I use two sets of sentences and human judgement data collected by Sprouse & Almeida, 2012, and Sprouse et al. 2013, that are representative of the field of generative syntax (Adger's \textit{Core Syntax}, 2003), have shaped current theories in generative syntax, and have been shown to be statistically powerful and reliably replicable experiments.  I find that BERT's MLM predictions only have a weak positive correlation with human acceptability judgements (0.30 and 0.32 on two datasets).

Next, I conduct a similar case study with BERT, but this time fine-tuning it on the Corpus of Linguistic Acceptability (CoLA, Warstadt et al. 2018), in order to use its learnt representations to emit acceptability judgements over sequences of words.  I once more compare these acceptability judgements to human judgement data, and examine further irregularities in the behavior of BERT's judgements in spite of the improved positive correlation (0.57 and 0.70 on two datasets).  I demonstrate that BERT's acceptability judgements are not sampled from the same distribution as the human acceptability judgements, and are therefore not driven by Knowledge of Syntax.

Lastly, I show via a final case study why the claim ($\mathrm{H}_A$) has not yet been proved in spite of multiple studies showing BERT has knowledge of different properties of syntax, such as Subject-Verb Agreement, for example.  I \textit{prime} BERT on 1 out of 5 syntactic phenomena from the Lasnik & Uriagereka 2003 textbook \textit{A Course in Minimalist Syntax} and show performance improvements of over 10\% classification accuracy as a result of only 20 sentences.  The experimental setup reveals learning dynamics that diverge from that of humans, and the peculiarities of the results reaffirm the null hypothesis ($\mathrm{H}_0$): BERT does not yet possess Knowledge of Syntax.

By the end of this thesis, you will understand the importance of $\mathrm{H}_0$ and why this should be the starting assumption of any and all linguistics experiments with Language Models.  Secondly, you will have seen strong evidence in favor of $\mathrm{H}_0$, and will know the importance of using well attested and validated human judgement data as the gold standard by which all Language Models should be measured.  Lastly, you will understand the limitations of Neural Language Models, and the necessary compromises in performance and accuracy that need to be made in order to apply them to any Natural Language task.

In addition to the aforementioned contributions, my hope in writing this thesis document is to spur a change in our approach to Natural Language Processing and Computational Linguistics: a stricter adherence to the scientific method as we move away from a behaviorist or empiricist theory of language and computation.