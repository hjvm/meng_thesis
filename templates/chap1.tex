%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Universal Function Approximators conquer Digital Infinity}

In this chapter, I will address reexamine the conventional definition of a Language Model as it pertains to human Natural Language.  Particularly, I will very briefly discuss the difference between competence and performance, and argue that Language Models are restricted to the performance side of Natural Language (<reference Revolutionary New Ideas...>).  Lastly, I examine NNs, touted as Universal Function Approximators in this context, and whether there exists a function that models human competence that can be approximated exclusively using human performance data..~\ref{ch1:sec}.

\section*{Grasping at straws: LMs and human performance.}

\subsection*{LMs output a probability distribution over sequences of words.}

Language Models (LMs) are, as the title of this section implies, computational devices that output a probability distribution when given a sequence of words.  How a LM achieves the probability distribution is immaterial to both this discussion and to the engineers that implement LMs to serve a practical purpose (i.e. Google predicting search queries or relevant advertisements).  Let it suffice to say that anything that could potentially improve the performance of a Language Model is fair game, such as taking into account whether it rained the previous Tuesday before making a prediction.  Typically however, LMs are trained on large corpora of natural language data in order to inform their predictions.  One of the most basic classes of such models are N-gram models, which rely on the preceding N words to determine the likelihood of the next word ($w_i$): $$\mathbf{P}(w_i|w_{i-N}, w_{i-(N+1)}, ..., w_{i-1})$$.  More sophisticated methods have since been invented and adopted.  Neural Networks have also been trained to output probabilities over sequences of words with varying objective and loss functions.  The common theme to all current LMs is that they require datasets of utterances of Natural Language in order to arrive at a satisfactory Probabiliy Mass Function (PMF) whose performance when assigning probabilities to discrete sequences of words is within specification.

\subsection*{An engineer's perspective of human competence vs. performance.}

Before delving into any arguments regarding Language Models, allow me to define the terms and clarify my stance regarding the human competence vs. performance distinction.  The competence vs. performance distinction is one I often relate to Physics: in school, one is taught to calculate the final vertical velocity ($\mathrm{v}_{y,final}$) of a perfectly round object when dropped from a height ($\mathrm{h}$) and having only the acceleration of gravity ($\mathrm{a}=\mathrm{g}\approx9.8\mathrm{m}/\mathrm{s}^2$).  In this setup, two objects of differing mass but dropped from the same height will impact the ground at the exact same time and at the exact same speed.  This fundamental principle is only revealed by the abstractions required to reduce the problem to the equation $$\mathrm{v}_{y,final} = \sqrt{\mathrm{h}*\mathrm{g}}$$.

If we studied this problem without abstracting away from wind resistance, accounting for terminal velocity, or calculating the effects of imperfections in the surface of the objects, we might arrive to the conclusion that smaller spheres tend to hit the ground faster than larger ones when dropped from the same height.  Such is also the case with human competence and performance as they pertain to Natural Language.

Human competence describes our KoL, which leads to our \textit{capability} to generate and understand infinitely long and infinitely nested, well-formed sequences of words.  Human performance is the realization of this capability once we take into account memory constraints, semantic plausibility, noise in the environment, among many other factors, that directly influence our externalization of Natural Language.

Human competence is what allows us to recognize a particular sequence of words as well-formed, in proper terms: grammatical.  A sequence of words being grammatical in a particular language is a categorical label describing whether that sequence of words is one that belongs to that language.  A sequence of words being acceptable is a gradient metric that determines whether that sequence of words \textit{makes sense}.  Think: if one were in a conversation and heard that sequence as part of the discourse, would one notice anything \textit{strange}?  If not, then it is an acceptable sequence, and whether or not it was grammatical, it flew under the radar.

Continuing the Physics example: How does one observe in a classroom the fact that two objects of equal mass fall at the same rate when dropped from equal height?  There is no (easily accessible) way of observing this without introducing the effects of wind resistance and other confounding variables, and such is also the case of human competence.  It cannot be observed except by probing human test subjects with sentences, which introduces the confounding variables that lead to human performance.  This limitation is the reason much theoretical work has been performed in order to directly relate human acceptability judgements to their presumed underlying grammaticality predictions.

Although the stance I have outlined in this section regarding competence and grammaticality vs. perofmrance and acceptability is relatively uncontroversial in Linguistics, no uncontested way of mapping acceptability judgements to grammaticality has been found.

\subsection{Can LMs learn from human performance?}
When it comes to Language Models, before any explicit mapping can be made between a sentence's grammaticality and acceptability, one needs a way of mapping the probability of that sentence to its acceptability; after all, the sentence (if it is an utterance) is an instance of human performance, not competence.  By extension, any and all training data used to inform Language Models' probabilistic predictions are subject to the same limitation: they reflect human performance, where all the variables have come into play.  Oversimplifying the matter: if LMs are trained on human performance data to produce a probability distribution, what impedes the reverse?  Why would one not be able to turn the probability of a sequence of words into its acceptability score?

This requires some careful attention to the definition of a LM: a model that outputs a probability distribution over a sequence of words.  Thus, LMs must obey the axioms of probability.  For this particular discussion, I will focus primarily on the second axiom: $$\mathrm{P}(\Omega)=1$$

If we consider that the probability distributions of LMs are informed almost exclusively by the dataset on which it was trained, it is not out of the realm of possibility to consider the collection of sequences of words in the dataset as the universe $\Omega$.  This then brings about the first large problem: any sequence not in $\Omega$ will never occur ($P(x \notin \Omega) = 0$).  A great deal of research has been conducted in order to find sophisticated \textit{smoothing} methods in order to assign nonzero probabilities to sequences of words not found in the original dataset.  This approach has its own limitations, which I will address in a later section.

Suppose a smoothing approach was discovered that effectively solved the problem of $x \notin \Omega$, then we may move on to the question of how to interpret the probabilities.  Intuitively, it would make sense that more acceptable sentences are more likely than less acceptable ones.  Even if not directly, if a sentence A ($s_\mathrm{A}$) has an acceptability of $\mathrm{Acc}(s_\mathrm{A}) = a_\mathrm{A}$ and a sentence B ($s_\mathrm{B}$) has an acceptability of $\mathrm{Acc}(s_\mathrm{B}) = a_\mathrm{B}$, the implication would take a form similar to the following:
$$\mathrm{If} a_\mathrm{A} > a_\mathrm{B} \Rightarrow \mathrm{P}(s_\mathrm{A}) > \mathrm{P}(s_\mathrm{B})$$
Yet this interpretation has dangerous consequences.  Considering that all probabilities are bounded between [0, 1]: this strongly biases the Probability Mass Function toward shorter sequences of words.  Consider the following example of coordination with two simple sentences:
\begin{itemize}
    \item $s_{\mathrm{A}}$: John and Mary do not leave the house.
    \item $s_{\mathrm{B}}$: John and Mary and Bill do not leave the house.
\end{itemize}
Both these sentences are perfectly acceptable, yet when calculating their probabilities, invariably $\mathrm{P}(s_\mathrm{A}) > \mathrm{P}(s_\mathrm{B})$ by the mere fact that $s_\mathrm{B}$ will be multiplied by more terms less than 1.  Despite this critical flaw, inequalities between probabilities are often used as a proxy for acceptability, and consequently as a metric to determine whether a language model possesses human-comparable KoL.  Two out of the many examples of this type of experimental setup are Linzen et al. 2016, and Hu et al. 2020.


Allow me to draw attention once again to $\mathrm{H}_0$: there exists no nonhuman computational device with human-comparable KoL.  Alternatively, let us assume there exists such a LM.  

\section*{Neural Networks: the perfect Language Models.}

\subsection*{Why NNs are the only LM capable of KoL.}
%The principal advantage of NLMs is that they are able to more effectively use the content of an entire sequence in order to compute the sequence probabilities.  Where an n-gram model is limited to the preceding n words, the NLM has multiple ways around the problem.  The most straightforward solution is to accept the entire sequence of tokens as input, such as in feed-forward Neural Networks.  A more sophisticated method involves processing each token in the sequence individually, but maintaining a a hidden state representation of the entire history of tokens seen so far, as used in Recursive Neural Networks (RNNs), Long-Short Term Memory networks (LSTMs), among others.  Lastly is what has been named the Masked Language Modeling (MLM) objective, used by Transformer networks.  MLM involves \textit{blocking out} one token in the entire sequence, and using the surrounding tokens to predict the missing token with an associated probability value.  I will only be concerned with bidirectional transformers...

Neural Networks are incredibly powerful statistical machines.  Under the assumption of arbitrary depth (infinite number of layers) or arbitrary width (infinite number of units per layer), it is said that NNs can approximate any continuous functions, hence their title of Universal Function Approximators.  Therefore, if there exists a probability distribution that perfectly models human Natural Language, or better yet, exhibits behavior stemming from human-comparable KoL, NNs will be able to find the Probability Mass Function that yields such a distribution.

With that out of the way, let us define what we would want from the ideal Language model by properly defining the bounds of the probability distribution.





Phasellus sed elit vehicula, gravida odio in, vulputate quam. Quisque in elit enim. Vivamus finibus justo elit, sed semper turpis aliquam porttitor. Nulla posuere bibendum nunc sit amet consequat. Vivamus commodo lorem sed metus fermentum rhoncus. Etiam porta sodales purus, vel aliquet lacus facilisis et. Etiam ornare velit non dui auctor fermentum. In elit augue, fringilla at lacinia at, facilisis sit amet lectus. Sed et hendrerit ex. Morbi tristique felis a augue egestas commodo. Nulla porttitor ut urna nec dignissim. Fusce ac pharetra risus, id rhoncus ligula. Pellentesque euismod viverra sem, vel porttitor libero blandit quis. Phasellus orci augue, mattis nec dolor ut, cursus mattis quam. Sed tincidunt eu metus sed pulvinar. Ut a nulla at leo semper accumsan efficitur eget leo.

Sed vel lectus ut dui tempor molestie. Suspendisse blandit sapien posuere quam tempor lobortis. Duis sollicitudin tincidunt dui, at aliquam lorem dictum sit amet. Aenean congue nibh lectus, ut faucibus turpis facilisis quis. Ut aliquet magna at placerat ultricies. Mauris convallis, risus efficitur gravida dapibus, lacus lorem malesuada ligula, eget porta diam felis non turpis. Nulla sed sem finibus, vehicula quam at, vulputate tellus\footnote{Here is a sample footnote referencing figures~\ref{arm:fig1}
and~\ref{arm:fig2}.}  

Quisque elit enim, molestie ac metus ut, condimentum convallis nibh:

\subsection{Subsection sample}

In tempus ex nibh, non eleifend risus iaculis ac. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Nullam in nisi eu arcu laoreet sollicitudin. Mauris consectetur venenatis arcu id finibus. Aenean pellentesque consectetur erat lacinia vulputate. Praesent tempus tempus lorem at dignissim. Proin at odio vitae tortor sollicitudin pretium. Quisque ac purus eu sem rutrum bibendum.

% This is an example of how you would use tgrind to include an example
% of source code; it is commented out in this template since the code
% example file does not exist. To use it, you need to remove the '%' on the
% beginning of the line, and insert your own information in the call.
%
%\tagrind[htbp]{code/pmn.s.tex}{Code sample}{opt:pmn}

Pellentesque ac leo eget lorem vulputate mattis eu a nisl. Duis elit erat, consectetur vulputate ullamcorper a, finibus quis turpis. Vivamus tincidunt dui id purus bibendum malesuada. Fusce accumsan, ipsum quis feugiat sodales, enim est aliquet leo, ut ornare justo mauris quis ex. Sed eros magna, suscipit et blandit non, pretium id felis. Praesent a vehicula tortor. Donec blandit dolor a ipsum sodales, eget aliquet nisl fermentum.

\subsection{Subsection with list}

Ut sollicitudin, lectus eget posuere porttitor, risus dui facilisis risus, a pharetra lacus elit vel eros. Proin fermentum accumsan mauris, quis posuere nisi pharetra scelerisque. 
\begin{enumerate}
  \item Item 1.
  \item Item 2.
  \item Item 3.
\end{enumerate}

Cras nec ullamcorper mauris. Aliquam erat volutpat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Suspendisse sed dui ac mi auctor scelerisque. Etiam at semper nisi. Cras nec dolor ac purus feugiat auctor. Nunc eget pulvinar massa.

% This is an example of how you would use tgrind to include an example
% of source code; it is commented out in this template since the code
% example file does not exist.  To use it, you need to remove the '%' on the
% beginning of the line, and insert your own information in the call.
%
%\tgrind[htbp]{code/be.s.tex}{Block Exponent}{opt:be}

\section{Section sample 3}

Quisque sed ultrices leo. Donec vestibulum auctor nibh, at faucibus libero mollis in. Quisque massa lorem, feugiat a lectus in, lobortis volutpat lectus. Donec accumsan dui erat, eu tempor tortor facilisis sed. Nulla ullamcorper augue et sapien dapibus, quis bibendum velit porta. Nullam mattis vehicula tortor porttitor porta. Interdum et malesuada fames ac ante ipsum primis in faucibus. Praesent suscipit, lorem vel viverra rhoncus, turpis orci dignissim dui, bibendum pulvinar justo sem vel lorem. Nam porttitor mollis tristique. Aliquam rhoncus magna quis nisl varius mattis. Sed rhoncus, diam in gravida iaculis, mauris tellus imperdiet turpis, at porttitor est leo vel velit. Praesent faucibus ornare sodales. Sed eu lorem purus.  

\subsection{Another subsection sample}

Nullam rhoncus posuere lacus, id volutpat nisi pulvinar viverra. Quisque quis ultricies ante. Duis sollicitudin sapien nec consequat vehicula. Vestibulum convallis erat in arcu aliquam eleifend. Nunc scelerisque lorem non luctus sodales. Curabitur eleifend odio et sagittis semper. Praesent sodales, diam nec vulputate iaculis, neque leo consectetur nunc, a luctus lacus purus et dui. Sed sit amet tortor ullamcorper, malesuada libero quis, imperdiet tortor. Cras tempor blandit massa, sit amet molestie sapien tincidunt quis. Nullam hendrerit venenatis massa, sed lacinia ligula tincidunt vitae.

Nam efficitur et lacus sed eleifend. Aenean quis ipsum eget leo ultrices ornare. Nullam rhoncus ante odio, at dignissim neque posuere eu. Pellentesque sodales tortor est, nec egestas sapien mollis quis. In lectus sapien, pellentesque congue erat consequat, hendrerit aliquet elit. Pellentesque eleifend purus ac diam bibendum, ac auctor ipsum posuere. Cras suscipit leo nec velit fermentum, id varius erat eleifend. Proin sagittis purus id ante lacinia, et congue eros tincidunt. Pellentesque at cursus tellus. Quisque id semper nunc. Quisque viverra a ex at ullamcorper. Morbi mollis erat at ex viverra fringilla. Proin ante dolor, dignissim sodales nisl ac, finibus egestas urna.

Nulla porta urna at pulvinar consectetur. Pellentesque suscipit, neque vitae ultricies rutrum, eros tellus iaculis dui, nec pulvinar justo nibh eu urna. Ut euismod massa nisi, et bibendum risus placerat quis. Integer pretium nulla id risus lobortis laoreet. Aenean quis quam fringilla, elementum odio non, lacinia purus. Vestibulum dui sapien, mollis sit amet massa vel, egestas faucibus velit. Phasellus non justo ut ante vestibulum dictum. Nam in nibh et libero malesuada aliquet. Donec in ex in magna luctus volutpat.

Sed quis dapibus libero. Curabitur id finibus nulla, sed semper felis. Proin dapibus nulla interdum, bibendum tortor et, blandit sapien. Etiam pretium tristique tortor non lacinia. Aliquam dapibus turpis lorem, sit amet porta ex dignissim vitae. In neque felis, sagittis sed ullamcorper lacinia, lobortis ut turpis. Nam quis aliquet justo. Nam eros mi, aliquam vel massa ac, ornare dignissim erat.  This is done by using some combination of
\begin{eqnarray*}
a_i & = & a_j + a_k \\
a_i & = & 2a_j + a_k \\
a_i & = & 4a_j + a_k \\
a_i & = & 8a_j + a_k \\
a_i & = & a_j - a_k \\
a_i & = & a_j \ll m \mbox{shift}
\end{eqnarray*}
instead of the multiplication.  For example, you could use:
\begin{eqnarray*}
r & = & 4s + s\\
r & = & r + r
\end{eqnarray*}
Or by xx:
\begin{eqnarray*}
t & = & 2s + s \\
r & = & 2t + s \\
r & = & 8r + t
\end{eqnarray*}
Cras pharetra ligula nec lectus bibendum, euismod mattis purus cursus. Nullam ut mi molestie purus ultricies lacinia. Phasellus sed orci ac lacus convallis vestibulum. Quisque id nulla ut ipsum finibus vehicula. Curabitur scelerisque erat lobortis, dapibus purus eget, faucibus sapien. Nam enim leo, faucibus id ante sed, fringilla luctus eros. Morbi vulputate, purus at commodo aliquet, turpis dolor sollicitudin libero, id vehicula risus dui sit amet nulla. Sed auctor efficitur urna. Praesent sagittis tellus ac velit vestibulum dignissim. Vivamus justo enim, pellentesque eu posuere id, mattis vitae felis. Aliquam id tincidunt diam. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas.